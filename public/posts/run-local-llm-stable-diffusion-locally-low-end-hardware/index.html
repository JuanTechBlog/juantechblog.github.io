<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Running LLM and Stable Diffusion Locally on My Low-End Hardware | Juan&#39;s Tech Blog</title>
<meta name="keywords" content="AI, LLM, Stable Diffusion 1.5, Llama 3.2 1B Q8, Llama 3.2 3B Q8, Jan AI, stable-diffusion.cpp, Vulkan, AVX2">
<meta name="description" content="Try running Llama 3.2 1B Q8, Llama 3.2 3B Q8 &amp; Stable Diffusion 1.5 on Intel UHD Graphics 630. Also, try to run AI models on the CPU.">
<meta name="author" content="">
<link rel="canonical" href="http://localhost:1313/posts/run-local-llm-stable-diffusion-locally-low-end-hardware/">
<meta name="google-site-verification" content="8tuuNRZtCnIRwkCljQTqQ_4KvsAJ-_jqNhFgewUSIFU">
<link crossorigin="anonymous" href="/assets/css/stylesheet.4599eadb9eb2ad3d0a8d6827b41a8fda8f2f4af226b63466c09c5fddbc8706b7.css" integrity="sha256-RZnq256yrT0KjWgntBqP2o8vSvImtjRmwJxf3byHBrc=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/posts/run-local-llm-stable-diffusion-locally-low-end-hardware/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-7634877819425351"
     crossorigin="anonymous"></script>
  
    
      <script async src="https://www.googletagmanager.com/gtag/js?id=G-VPQZ6L0MQS"></script>
      <script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-VPQZ6L0MQS');
        }
      </script>
    
  

<meta property="og:title" content="Running LLM and Stable Diffusion Locally on My Low-End Hardware" />
<meta property="og:description" content="Try running Llama 3.2 1B Q8, Llama 3.2 3B Q8 &amp; Stable Diffusion 1.5 on Intel UHD Graphics 630. Also, try to run AI models on the CPU." />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://localhost:1313/posts/run-local-llm-stable-diffusion-locally-low-end-hardware/" />
<meta property="og:image" content="http://localhost:1313/posts/run-local-llm-stable-diffusion-locally-low-end-hardware/imgs/run-llms-and-stable-diffusion-locally-on-intel-uhd-630.webp" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-10-28T00:00:00+00:00" />
<meta property="article:modified_time" content="2024-10-28T00:00:00+00:00" /><meta property="og:site_name" content="Juan&#39;s Tech Blog" />

<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="http://localhost:1313/posts/run-local-llm-stable-diffusion-locally-low-end-hardware/imgs/run-llms-and-stable-diffusion-locally-on-intel-uhd-630.webp" />
<meta name="twitter:title" content="Running LLM and Stable Diffusion Locally on My Low-End Hardware"/>
<meta name="twitter:description" content="Try running Llama 3.2 1B Q8, Llama 3.2 3B Q8 &amp; Stable Diffusion 1.5 on Intel UHD Graphics 630. Also, try to run AI models on the CPU."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "http://localhost:1313/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Running LLM and Stable Diffusion Locally on My Low-End Hardware",
      "item": "http://localhost:1313/posts/run-local-llm-stable-diffusion-locally-low-end-hardware/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Running LLM and Stable Diffusion Locally on My Low-End Hardware",
  "name": "Running LLM and Stable Diffusion Locally on My Low-End Hardware",
  "description": "Try running Llama 3.2 1B Q8, Llama 3.2 3B Q8 \u0026 Stable Diffusion 1.5 on Intel UHD Graphics 630. Also, try to run AI models on the CPU.",
  "keywords": [
    "AI", "LLM", "Stable Diffusion 1.5", "Llama 3.2 1B Q8", "Llama 3.2 3B Q8", "Jan AI", "stable-diffusion.cpp", "Vulkan", "AVX2"
  ],
  "articleBody": "Let’s start with my PC spec.\nComponents Parts Processor Intel i5 10400ES iGPU Intel UHD Graphics 630 Motherboard AsRock B460M Pro RAM Adata DDR4-2666 4GB x2 dGPU N/A When it comes to AI, most people probably think it requires a beefy GPU to run smoothly and quickly. And that’s true. But just how bad will it be when running AI on a low-end GPU?\nHow to run LLM and Stable Diffusion on Intel UHD 630 iGPU? Intel has a Pytouch library called IPEX-LLM for running LLM on Intel CPU and GPU. However, it doesn’t seem to support my iGPU.\nThe IPEX-LLM requires a graphic driver version of 31.0.101.5122 and higher, which is only compatible with Intel Core 11 gen and newer iGPUs, as well as Intel Arc GPUs. For the Intel UHD 630, the latest driver available is 31.0.101.2130—below the required level.\nSo, I’m looking for alternative options. I think Vulkan might be the only way to get LLM and Stable Diffusion running on my GPU.\nLLM Test Run (Llama 3.2 1B Instruct Q8 Llama 3.2 3B Instruct Q8) Let’s run some Large Language Model(LLM). I used Jan, a software that support Vulkan, to run open LLMs. I will test run the Llama 3.2 1B Instruct Q8 and Llama 3.2 3B Instruct Q8.\nDownload and install Jan. Go the Settings → Advanced Settings → toggle Experimental Mode → toggle Vulkan Support. Go the Hub, download Llama 3.2 1B Instruct Q8 and Llama 3.2 3B Instruct Q8. Then, we can start using these models. First Attempt on iGPU I run the Llama 3.2 1B Instruct Q8 on my iGPU and asked it, “List down what you can do for me”.\nThe iGPU utilization almost hit 100% while generating the response.\nThe token speed is around 7.7t/s, which is fairly usable.\nSecond Attempt on iGPU Next, I switch the model to Llama 3.2 3B Instruct Q8 and run it.\nThe model can't even start—it failed to allocate memory due to insufficient device memory, as noted in the log.\nThe Llama 3.2 3B Instruct Q8 require at least 8GB VRAM. My iGPU only got 4GB shared VRAM. Obviously, I don’t have enough RAM to share with my iGPU.\nThird Attempt on CPU Why not try the CPU instead? I toggle off the Vulkan support and run Llama 3.2 1B Instruct Q8 on the CPU.\nCPU utilization is between 80% and 90% during response generation.\nWoW! The token speed is double up to 14.46t/s, which is faster than the iGPU.\nFourth Attempt on CPU I continue try the Llama 3.2 3B Instruct Q8 on the CPU. This time, the model able to start and generate response.\nCPU utilization close to 100% while generating response.\nThe token speed is about 5.56t/s, which is a bit slow but still usable.\nStable Diffusion Test Run (SD 1.5) To run Stable Diffusion on the iGPU, I use the stable-diffusion.cpp version that supports Vulkan.\nFirst Attempt on iGPU I run the Vulkan-supported stable-diffusion.cpp executable with Stable Diffusion 1.5 model and using a simple prompt, \"people\", with default settings.\nThe iGPU utilization is around 100%.\nIt takes 551.46s(≈ 9 minutes and 11 seconds) to complete and the speed is 28.09s/it.\nSecond Attempt on CPU - AVX2 I then download the CPU AVX2-supported stable-diffusion.cpp. I run it with Stable Diffusion 1.5 model and a simple prompt, \"people\". All the settings remain default.\nThe CPU utilization is around 66% only.\nIt take 352.32s(≈ 5 minutes and 52 seconds) to complete with the speed of 16.41s/it.—nearly twice as fast as the iGPU.\nConclusion Thanks to Vulkan, I can play a little bit of AI things on my low-end iGPU. However, when looking at token speed or iteration times, the CPU outperforms the iGPU by nearly double, which making almost no sense to use iGPU for AI tasks.\nThe only use case I can imagine is offload some of the workload from the CPU . For example, if I’m coding and need a LLM to help debug, I could let the iGPU handle the LLM responses while the CPU focuses on compiling code.\nLastly, it’s quite fun to play around AI.\n",
  "wordCount" : "687",
  "inLanguage": "en",
  "image":"http://localhost:1313/posts/run-local-llm-stable-diffusion-locally-low-end-hardware/imgs/run-llms-and-stable-diffusion-locally-on-intel-uhd-630.webp","datePublished": "2024-10-28T00:00:00Z",
  "dateModified": "2024-10-28T00:00:00Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://localhost:1313/posts/run-local-llm-stable-diffusion-locally-low-end-hardware/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Juan's Tech Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "http://localhost:1313/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="Juan&#39;s Tech Blog (Alt + H)">Juan&#39;s Tech Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                <ul class="lang-switch"><li>|</li>
                </ul>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="http://localhost:1313/">Home</a>&nbsp;»&nbsp;<a href="http://localhost:1313/posts/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      Running LLM and Stable Diffusion Locally on My Low-End Hardware
    </h1>
    <div class="post-description">
      Try running Llama 3.2 1B Q8, Llama 3.2 3B Q8 &amp; Stable Diffusion 1.5 on Intel UHD Graphics 630. Also, try to run AI models on the CPU.
    </div>
    <div class="post-meta"><span title='2024-10-28 00:00:00 +0000 UTC'>October 28, 2024</span>&nbsp;·&nbsp;4 min&nbsp;·&nbsp;687 words

</div>
  </header> 
<figure class="entry-cover">
        <img loading="eager" src="http://localhost:1313/posts/run-local-llm-stable-diffusion-locally-low-end-hardware/imgs/run-llms-and-stable-diffusion-locally-on-intel-uhd-630.webp" alt="Run LLMs and Stable Diffusion Locally on Intel UHD 630">
        <p>Run LLMs and Stable Diffusion Locally on Intel UHD 630</p>
</figure><div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><nav id="TableOfContents">
  <ul>
    <li><a href="#how-to-run-llm-and-stable-diffusion-on-intel-uhd-630-igpu">How to run LLM and Stable Diffusion on Intel UHD 630 iGPU?</a></li>
    <li><a href="#llm-test-run-llama-32-1b-instruct-q8-llama-32-3b-instruct-q8">LLM Test Run (Llama 3.2 1B Instruct Q8 Llama 3.2 3B Instruct Q8)</a>
      <ul>
        <li><a href="#first-attempt-on-igpu">First Attempt on iGPU</a></li>
        <li><a href="#second-attempt-on-igpu">Second Attempt on iGPU</a></li>
        <li><a href="#third-attempt-on-cpu">Third Attempt on CPU</a></li>
        <li><a href="#fourth-attempt-on-cpu">Fourth Attempt on CPU</a></li>
      </ul>
    </li>
    <li><a href="#stable-diffusion-test-run-sd-15">Stable Diffusion Test Run (SD 1.5)</a>
      <ul>
        <li><a href="#first-attempt-on-igpu-1">First Attempt on iGPU</a></li>
        <li><a href="#second-attempt-on-cpu---avx2">Second Attempt on CPU - AVX2</a></li>
      </ul>
    </li>
    <li><a href="#conclusion">Conclusion</a></li>
  </ul>
</nav>
        </div>
    </details>
</div>

  <div class="post-content"><p>Let&rsquo;s start with my PC spec.</p>
<table>
<thead>
<tr>
<th>Components</th>
<th>Parts</th>
</tr>
</thead>
<tbody>
<tr>
<td>Processor</td>
<td>Intel i5 10400ES</td>
</tr>
<tr>
<td>iGPU</td>
<td><code>Intel UHD Graphics 630</code></td>
</tr>
<tr>
<td>Motherboard</td>
<td>AsRock B460M Pro</td>
</tr>
<tr>
<td>RAM</td>
<td>Adata DDR4-2666 4GB x2</td>
</tr>
<tr>
<td>dGPU</td>
<td>N/A</td>
</tr>
</tbody>
</table>
<p>When it comes to AI, most people probably think it requires a beefy GPU to run smoothly and quickly.
And that&rsquo;s true.
But just how bad will it be when running AI on a low-end GPU?</p>
<h2 id="how-to-run-llm-and-stable-diffusion-on-intel-uhd-630-igpu">How to run LLM and Stable Diffusion on Intel UHD 630 iGPU?<a hidden class="anchor" aria-hidden="true" href="#how-to-run-llm-and-stable-diffusion-on-intel-uhd-630-igpu">#</a></h2>
<p>Intel has a Pytouch library called <code>IPEX-LLM</code> for running LLM on Intel CPU and GPU.
However, it doesn’t seem to support my iGPU.</p>
<p>The IPEX-LLM requires a graphic driver version of <code>31.0.101.5122</code> and higher, which is only compatible with Intel Core 11 gen and newer iGPUs, as well as Intel Arc GPUs. For the Intel UHD 630, the latest driver available is <code>31.0.101.2130</code>—below the required level.</p>
<p>So, I’m looking for alternative options. I think Vulkan might be the only way to get LLM and Stable Diffusion running on my GPU.</p>
<h2 id="llm-test-run-llama-32-1b-instruct-q8-llama-32-3b-instruct-q8">LLM Test Run (Llama 3.2 1B Instruct Q8 Llama 3.2 3B Instruct Q8)<a hidden class="anchor" aria-hidden="true" href="#llm-test-run-llama-32-1b-instruct-q8-llama-32-3b-instruct-q8">#</a></h2>
<p>Let&rsquo;s run some Large Language Model(LLM).
I used Jan, a software that support Vulkan, to run open LLMs.
I will test run the <code>Llama 3.2 1B Instruct Q8</code> and <code>Llama 3.2 3B Instruct Q8</code>.</p>
<ol>
<li>Download and install <a href="https://jan.ai/">Jan</a>.</li>
<li>Go the Settings → Advanced Settings → toggle Experimental Mode → toggle Vulkan Support.</li>
<li>Go the Hub, download <code>Llama 3.2 1B Instruct Q8</code> and <code>Llama 3.2 3B Instruct Q8</code>.</li>
<li>Then, we can start using these models.</li>
</ol>
<h3 id="first-attempt-on-igpu">First Attempt on iGPU<a hidden class="anchor" aria-hidden="true" href="#first-attempt-on-igpu">#</a></h3>
<p>I run the <code>Llama 3.2 1B Instruct Q8</code> on my iGPU and asked it, &ldquo;List down what you can do for me&rdquo;.</p>
<p><img loading="lazy" src="./imgs/jan-ai-gpu-vulkan-llama-3-2-1b-q8-usage.webp" alt="Intel UHD 630 utilization stats while running Llama 3.2 1B Instruct Q8 on iGPU using Vulkan and Jan AI "  />

The iGPU utilization almost hit 100% while generating the response.</p>
<p><img loading="lazy" src="./imgs/jan-ai-gpu-vulkan-llama-3-2-1b-q8.webp" alt="Running Llama 3.2 1B Instruct Q8 on iGPU using Vulkan and Jan AI"  />

The token speed is around <code>7.7t/s</code>, which is fairly usable.</p>
<h3 id="second-attempt-on-igpu">Second Attempt on iGPU<a hidden class="anchor" aria-hidden="true" href="#second-attempt-on-igpu">#</a></h3>
<p>Next, I switch the model to <code>Llama 3.2 3B Instruct Q8</code> and run it.</p>
<p><img loading="lazy" src="./imgs/jan-ai-cpu-llama-3-2-3b-q8-out-of-device-memory.webp" alt="Error message while running Llama 3.2 32sB Instruct Q8 on iGPU using Vulkan and Jan AI"  />

<code>The model can't even start</code>—it failed to allocate memory due to <code>insufficient device memory</code>, as noted in the log.</p>
<p>The <a href="https://llamaimodel.com/requirements-3-2/#3B">Llama 3.2 3B Instruct Q8 require</a> at least 8GB VRAM. My iGPU only got 4GB shared VRAM. Obviously, I don&rsquo;t have enough RAM to share with my iGPU.</p>
<h3 id="third-attempt-on-cpu">Third Attempt on CPU<a hidden class="anchor" aria-hidden="true" href="#third-attempt-on-cpu">#</a></h3>
<p>Why not try the CPU instead? I toggle off the Vulkan support and run <code>Llama 3.2 1B Instruct Q8</code> on the <code>CPU</code>.</p>
<p><img loading="lazy" src="./imgs/jan-ai-cpu-llama-3-2-1b-q8-usage.webp" alt="CPU utilization stats while running Llama 3.2 1B Instruct Q8 on CPU using Jan AI"  />

CPU utilization is between 80% and 90% during response generation.</p>
<p><img loading="lazy" src="./imgs/jan-ai-cpu-llama-3-2-1b-q8.webp" alt="Running Llama 3.2 1B Instruct Q8 on CPU using Jan AI "  />

WoW! The token speed is double up to <code>14.46t/s</code>, which is faster than the iGPU.</p>
<h3 id="fourth-attempt-on-cpu">Fourth Attempt on CPU<a hidden class="anchor" aria-hidden="true" href="#fourth-attempt-on-cpu">#</a></h3>
<p>I continue try the Llama 3.2 3B Instruct Q8 on the CPU. This time, the model able to start and generate response.</p>
<p><img loading="lazy" src="./imgs/jan-ai-cpu-llama-3-2-3b-q8-usage.webp" alt="CPU utilization stats while running Llama 3.2 3B Instruct Q8 on CPU using Jan AI"  />

CPU utilization close to 100% while generating response.</p>
<p><img loading="lazy" src="./imgs/jan-ai-cpu-llama-3-2-3b-q8.webp" alt="Running Llama 3.2 3B Instruct Q8 on CPU using Jan AI"  />

The token speed is about 5.56t/s, which is a bit slow but still usable.</p>
<h2 id="stable-diffusion-test-run-sd-15">Stable Diffusion Test Run (SD 1.5)<a hidden class="anchor" aria-hidden="true" href="#stable-diffusion-test-run-sd-15">#</a></h2>
<p>To run Stable Diffusion on the iGPU, I use the <code>stable-diffusion.cpp</code> version that supports <code>Vulkan</code>.</p>
<h3 id="first-attempt-on-igpu-1">First Attempt on iGPU<a hidden class="anchor" aria-hidden="true" href="#first-attempt-on-igpu-1">#</a></h3>
<p>I run the <code>Vulkan-supported stable-diffusion.cpp</code> executable with <code>Stable Diffusion 1.5 model</code> and using a <code>simple prompt, &quot;people&quot;</code>, with <code>default settings</code>.</p>
<p><img loading="lazy" src="./imgs/stable-diffusion-cpp-gpu-vulkan-usage.webp" alt="iGPU utilization stats while running Stable Diffusion 1.5 using stable-diffusion.cpp and Vulkan on iGPU"  />

The iGPU utilization is around 100%.</p>
<p><img loading="lazy" src="./imgs/stable-diffusion-cpp-gpu-vulkan-cli-prompt.webp" alt="Running Stable Diffusion 1.5 using stable-diffusion.cpp and Vulkan on iGPU"  />

It takes 551.46s(≈ 9 minutes and 11 seconds) to complete and the speed is 28.09s/it.</p>
<h3 id="second-attempt-on-cpu---avx2">Second Attempt on CPU - AVX2<a hidden class="anchor" aria-hidden="true" href="#second-attempt-on-cpu---avx2">#</a></h3>
<p>I then download the <code>CPU AVX2-supported</code> stable-diffusion.cpp. I run it with <code>Stable Diffusion 1.5</code> model and a <code>simple prompt, &quot;people&quot;</code>. All the <code>settings remain default</code>.</p>
<p><img loading="lazy" src="./imgs/stable-diffusion-cpp-cpu-avx2-usage.webp" alt="CPU utilization stats while running Stable Diffusion 1.5 using stable-diffusion.cpp and AVX2 on CPU"  />

The CPU utilization is around 66% only.</p>
<p><img loading="lazy" src="./imgs/stable-diffusion-cpp-cpu-avx2-cli-prompt.webp" alt="running Stable Diffusion 1.5 using stable-diffusion.cpp and AVX2 on CPU"  />

It take 352.32s(≈ 5 minutes and 52 seconds) to complete with the speed of 16.41s/it.—nearly twice as fast as the iGPU.</p>
<h2 id="conclusion">Conclusion<a hidden class="anchor" aria-hidden="true" href="#conclusion">#</a></h2>
<p>Thanks to Vulkan, I can play a little bit of AI things on my low-end iGPU.
However, when looking at token speed or iteration times, the CPU outperforms the iGPU by nearly double, which making almost no sense to use iGPU for AI tasks.</p>
<p>The only use case I can imagine is offload some of the workload from the CPU .
For example, if I&rsquo;m coding and need a LLM to help debug, I could let the iGPU handle the LLM responses while the CPU focuses on compiling code.</p>
<p>Lastly, it&rsquo;s quite fun to play around AI.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="http://localhost:1313/tags/ai/">AI</a></li>
      <li><a href="http://localhost:1313/tags/llm/">LLM</a></li>
      <li><a href="http://localhost:1313/tags/stable-diffusion-1.5/">Stable Diffusion 1.5</a></li>
      <li><a href="http://localhost:1313/tags/llama-3.2-1b-q8/">Llama 3.2 1B Q8</a></li>
      <li><a href="http://localhost:1313/tags/llama-3.2-3b-q8/">Llama 3.2 3B Q8</a></li>
      <li><a href="http://localhost:1313/tags/jan-ai/">Jan AI</a></li>
      <li><a href="http://localhost:1313/tags/stable-diffusion.cpp/">Stable-Diffusion.cpp</a></li>
      <li><a href="http://localhost:1313/tags/vulkan/">Vulkan</a></li>
      <li><a href="http://localhost:1313/tags/avx2/">AVX2</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="http://localhost:1313/posts/create-telegram-bot-with-cloudflare-workers-kv/">
    <span class="title">« Prev</span>
    <br>
    <span>Create a Telegram Bot using JavaScript, Cloudflare Workers and KV.</span>
  </a>
  <a class="next" href="http://localhost:1313/posts/home-asssitant-remote-access-cloudflare-tunnel-pp-ua-domain/">
    <span class="title">Next »</span>
    <br>
    <span>How to Access Home Assistant Remotely.</span>
  </a>
</nav>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Running LLM and Stable Diffusion Locally on My Low-End Hardware on x"
            href="https://x.com/intent/tweet/?text=Running%20LLM%20and%20Stable%20Diffusion%20Locally%20on%20My%20Low-End%20Hardware&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2frun-local-llm-stable-diffusion-locally-low-end-hardware%2f&amp;hashtags=AI%2cLLM%2cStableDiffusion1.5%2cLlama3.21BQ8%2cLlama3.23BQ8%2cJanAI%2cstable-diffusion.cpp%2cVulkan%2cAVX2">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Running LLM and Stable Diffusion Locally on My Low-End Hardware on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2frun-local-llm-stable-diffusion-locally-low-end-hardware%2f&amp;title=Running%20LLM%20and%20Stable%20Diffusion%20Locally%20on%20My%20Low-End%20Hardware&amp;summary=Running%20LLM%20and%20Stable%20Diffusion%20Locally%20on%20My%20Low-End%20Hardware&amp;source=http%3a%2f%2flocalhost%3a1313%2fposts%2frun-local-llm-stable-diffusion-locally-low-end-hardware%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Running LLM and Stable Diffusion Locally on My Low-End Hardware on reddit"
            href="https://reddit.com/submit?url=http%3a%2f%2flocalhost%3a1313%2fposts%2frun-local-llm-stable-diffusion-locally-low-end-hardware%2f&title=Running%20LLM%20and%20Stable%20Diffusion%20Locally%20on%20My%20Low-End%20Hardware">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Running LLM and Stable Diffusion Locally on My Low-End Hardware on facebook"
            href="https://facebook.com/sharer/sharer.php?u=http%3a%2f%2flocalhost%3a1313%2fposts%2frun-local-llm-stable-diffusion-locally-low-end-hardware%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Running LLM and Stable Diffusion Locally on My Low-End Hardware on whatsapp"
            href="https://api.whatsapp.com/send?text=Running%20LLM%20and%20Stable%20Diffusion%20Locally%20on%20My%20Low-End%20Hardware%20-%20http%3a%2f%2flocalhost%3a1313%2fposts%2frun-local-llm-stable-diffusion-locally-low-end-hardware%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Running LLM and Stable Diffusion Locally on My Low-End Hardware on telegram"
            href="https://telegram.me/share/url?text=Running%20LLM%20and%20Stable%20Diffusion%20Locally%20on%20My%20Low-End%20Hardware&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2frun-local-llm-stable-diffusion-locally-low-end-hardware%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Running LLM and Stable Diffusion Locally on My Low-End Hardware on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Running%20LLM%20and%20Stable%20Diffusion%20Locally%20on%20My%20Low-End%20Hardware&u=http%3a%2f%2flocalhost%3a1313%2fposts%2frun-local-llm-stable-diffusion-locally-low-end-hardware%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer><script src="https://giscus.app/client.js"
        data-repo="JuanTechBlog/juantechblog.github.io"
        data-repo-id="R_kgDOLHpjjw"
        data-category="General"
        data-category-id="DIC_kwDOLHpjj84Ccp5u"
        data-mapping="url"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="top"
        data-theme="preferred_color_scheme"
        data-lang="en"
        data-loading="lazy"
        crossorigin="anonymous"
        async>
</script>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2024 <a href="http://localhost:1313/">Juan&#39;s Tech Blog</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
